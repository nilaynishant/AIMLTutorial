{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT5BI3RvMVauGHprXsMb/8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilaynishant/AIMLTutorial/blob/main/11_Basic_foundation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Geospatial Foundation Model Training (Prithvi-inspired)\n",
        "# Uses EuroSAT dataset for remote sensing\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision einops timm pillow matplotlib scikit-learn tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EuroSAT\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ====================\n",
        "# 1. CONFIGURATION\n",
        "# ====================\n",
        "class Config:\n",
        "    # Model parameters\n",
        "    img_size = 64  # Smaller for faster training\n",
        "    patch_size = 8\n",
        "    in_channels = 3\n",
        "    embed_dim = 256  # Smaller than full Prithvi\n",
        "    depth = 6  # Fewer transformer blocks\n",
        "    num_heads = 8\n",
        "    mlp_ratio = 4\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 32\n",
        "    num_epochs = 20\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 0.05\n",
        "    warmup_epochs = 2\n",
        "\n",
        "    # Masking parameters (for self-supervised learning)\n",
        "    mask_ratio = 0.75\n",
        "\n",
        "    # Data\n",
        "    data_root = './data'\n",
        "    num_workers = 2\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "SJKXcw1z3wBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0k5O1413fDg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ====================\n",
        "# 2. VISION TRANSFORMER (Encoder-Decoder)\n",
        "# ====================\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them\"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
        "                             kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W) -> (B, num_patches, embed_dim)\n",
        "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class MaskedAutoencoder(nn.Module):\n",
        "    \"\"\"Masked Autoencoder for self-supervised pre-training\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Encoder\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            config.img_size, config.patch_size,\n",
        "            config.in_channels, config.embed_dim\n",
        "        )\n",
        "\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config.embed_dim))\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.embed_dim, config.num_heads, config.mlp_ratio)\n",
        "            for _ in range(config.depth)\n",
        "        ])\n",
        "        self.encoder_norm = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_embed_dim = config.embed_dim // 2\n",
        "        self.decoder_embed = nn.Linear(config.embed_dim, decoder_embed_dim)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, decoder_embed_dim)\n",
        "        )\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            TransformerBlock(decoder_embed_dim, config.num_heads, config.mlp_ratio)\n",
        "            for _ in range(config.depth // 2)\n",
        "        ])\n",
        "        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n",
        "\n",
        "        # Predict pixel values\n",
        "        self.decoder_pred = nn.Linear(\n",
        "            decoder_embed_dim,\n",
        "            config.patch_size ** 2 * config.in_channels\n",
        "        )\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize pos_embed\n",
        "        torch.nn.init.normal_(self.pos_embed, std=0.02)\n",
        "        torch.nn.init.normal_(self.decoder_pos_embed, std=0.02)\n",
        "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"Random masking following MAE\"\"\"\n",
        "        B, N, D = x.shape\n",
        "        len_keep = int(N * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(B, N, device=x.device)\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        mask = torch.ones([B, N], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        for blk in self.encoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.encoder_norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        mask_tokens = self.mask_token.repeat(\n",
        "            x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1\n",
        "        )\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        x = self.decoder_pred(x)\n",
        "        x = x[:, 1:, :]  # Remove cls token\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)\n",
        "        return pred, mask\n",
        "\n",
        "    def patchify(self, imgs):\n",
        "        \"\"\"Convert images to patches\"\"\"\n",
        "        p = self.config.patch_size\n",
        "        h = w = self.config.img_size // p\n",
        "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p)\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
        "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3)\n",
        "        return x\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"Convert patches back to images\"\"\"\n",
        "        p = self.config.patch_size\n",
        "        h = w = self.config.img_size // p\n",
        "        x = x.reshape(x.shape[0], h, w, p, p, 3)\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        x = x.reshape(x.shape[0], 3, h * p, w * p)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# 3. DATASET PREPARATION\n",
        "# ====================\n",
        "# Download EuroSAT dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(config.img_size),\n",
        "    transforms.CenterCrop(config.img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Downloading EuroSAT dataset...\")\n",
        "dataset = EuroSAT(root=config.data_root, download=True, transform=transform)\n",
        "\n",
        "# Split into train/val\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size,\n",
        "                          shuffle=True, num_workers=config.num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size,\n",
        "                        shuffle=False, num_workers=config.num_workers)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HSN8V1gc3sE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# 4. TRAINING SETUP\n",
        "# ====================\n",
        "model = MaskedAutoencoder(config).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate,\n",
        "                        weight_decay=config.weight_decay)\n",
        "\n",
        "# Cosine learning rate schedule with warmup\n",
        "def adjust_learning_rate(optimizer, epoch, config):\n",
        "    if epoch < config.warmup_epochs:\n",
        "        lr = config.learning_rate * (epoch + 1) / config.warmup_epochs\n",
        "    else:\n",
        "        lr = config.learning_rate * 0.5 * (1 + np.cos(\n",
        "            np.pi * (epoch - config.warmup_epochs) /\n",
        "            (config.num_epochs - config.warmup_epochs)\n",
        "        ))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def compute_loss(model, imgs, mask_ratio):\n",
        "    pred, mask = model(imgs, mask_ratio)\n",
        "    target = model.patchify(imgs)\n",
        "\n",
        "    # MSE loss only on masked patches\n",
        "    loss = (pred - target) ** 2\n",
        "    loss = loss.mean(dim=-1)\n",
        "    loss = (loss * mask).sum() / mask.sum()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "R2LXQn553pz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# 5. TRAINING LOOP\n",
        "# ====================\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(config.num_epochs):\n",
        "    # Adjust learning rate\n",
        "    lr = adjust_learning_rate(optimizer, epoch, config)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.num_epochs}')\n",
        "\n",
        "    for imgs, _ in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = compute_loss(model, imgs, config.mask_ratio)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{lr:.6f}'})\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            loss = compute_loss(model, imgs, config.mask_ratio)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "hjRBHVju3lMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# 6. VISUALIZATION\n",
        "# ====================\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize reconstructions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a batch\n",
        "    imgs, _ = next(iter(val_loader))\n",
        "    imgs = imgs[:4].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    pred, mask = model(imgs, config.mask_ratio)\n",
        "    pred = model.unpatchify(pred)\n",
        "\n",
        "    # Denormalize for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "\n",
        "    imgs_vis = imgs * std + mean\n",
        "    pred_vis = pred * std + mean\n",
        "\n",
        "    # Create masked images\n",
        "    mask = mask.unsqueeze(-1).repeat(1, 1, config.patch_size**2 * 3)\n",
        "    mask = model.unpatchify(mask)\n",
        "    imgs_masked = imgs * (1 - mask) + mask\n",
        "    imgs_masked_vis = imgs_masked * std + mean\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
        "    for i in range(4):\n",
        "        # Original\n",
        "        axes[i, 0].imshow(imgs_vis[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
        "        axes[i, 0].set_title('Original')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Masked\n",
        "        axes[i, 1].imshow(imgs_masked_vis[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
        "        axes[i, 1].set_title(f'Masked ({config.mask_ratio*100:.0f}%)')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Reconstruction\n",
        "        axes[i, 2].imshow(pred_vis[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
        "        axes[i, 2].set_title('Reconstruction')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k99WjL9d3h5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# 7. SAVE MODEL\n",
        "# ====================\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': config,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses\n",
        "}, 'geospatial_foundation_model.pth')\n",
        "\n",
        "print(\"\\nModel saved as 'geospatial_foundation_model.pth'\")\n",
        "print(\"\\nTo use this model for downstream tasks (classification, segmentation, etc.),\")\n",
        "print(\"you can load the encoder weights and fine-tune on your specific task!\")"
      ],
      "metadata": {
        "id": "72W__1pc3hTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}